{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最开始的部分和第一篇讲的seq2seq模型一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "# UNK = 2\n",
    "# GO  = 3\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.truncated_normal([vocab_size, input_embedding_size], mean=0.0, stddev=0.1), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder\n",
    "此处定义的encoder和第一篇中的不同，需要注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于LSTMCell和 BasicLSTMCell 的区别，这个可以去tf的官网看说明：\n",
    "\n",
    "> It(BasicLSTMCell) does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "\n",
    "这个projection layer就是在输出ouput上加了一层fc，将其作为lstm的输出\n",
    "\n",
    "cell clipping ?\n",
    "\n",
    "peep-hole connections?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处 encoder_cell 的形状是：[max_time, batch_size, ...]，\n",
    "\n",
    "encoder_fw_outputs 的形状是：[max_time, batch_size, cell_fw.output_size]\n",
    "\n",
    "sequence_length：如果没有提供，则默认长度就是 [0,max_time-1]，如果提供了则取 [0,sequence_length-1]\n",
    "\n",
    "state 的形状是 ： [batch_size, cell_fw.state_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=20, h=20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder_fw_final_state.h is activations of hidden layer of LSTM cell\n",
    "- encoder_fw_final_state.c is final output, which can potentially be transfromed with some wrapper \n",
    "\n",
    "\n",
    "此处 h 指的是内部的状态，而 c 则是 h 经过 activations 后的状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，我们不会舍去 encoder_outputs，而是会将其用于attention机制\n",
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步我们要要决定 decoder 运行多少步后结束，有两种策略\n",
    "\n",
    "- Stop after specified number of unrolling steps\n",
    "- Stop after model produced\n",
    "\n",
    "此处我们选择第一种，固定的步骤，在之前的一些tutorial中一般是len(encoder_input)+10，此处我们测试用则简单点使用len(encoder_input)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output projection\n",
    "decoder的预测流程是：\n",
    "\n",
    "> output(t) -> output projection(t) -> prediction(t) (argmax) -> input embedding(t+1) -> input(t+1)\n",
    "\n",
    "我们先指定输出projection的W和b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([decoder_hidden_units, vocab_size], 0, 0.1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder via tf.nn.raw_rnn\n",
    "使用dynamic_rnn有一些限制，不能让我们自定义输入，像下图一样\n",
    "\n",
    "![raw_rnn](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2016/04/nct-seq2seq.png)\n",
    "\n",
    "\n",
    "图片来自： http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于标准的 tf.nn.dynamic_rnn ，其输入 (t, ..., t+n) 需要事先作为一个 Tensor 输入，其动态 \"Dynamic\" 的含义是 n 的大小在每个batch中可以改变\n",
    "\n",
    "此时如果我们希望一些更加复杂的机制，如每个cell的输出作为下一个的输入，或者实现 soft attention ，就没有办法了，这个时候我们就转向 tf.nn.raw_rnn 函数了\n",
    "\n",
    "tf.nn.raw_rnn 最重要的就是 loop_fn 函数的编写，loop_fn做了一个映射\n",
    "\n",
    "> (time, previous_cell_output, previous_cell_state, previous_loop_state) -> (elements_finished, input, cell_state, output, loop_state).\n",
    "\n",
    "上面转换的时机是在调用rnncell之前，准备好输入\n",
    "\n",
    "loop_fn调用的时机有2个：\n",
    "1. Initial call at time=0 to provide initial cell_state and input to RNN.\n",
    "2. Transition call for all following timesteps where you define transition between two adjacent steps.\n",
    "\n",
    "下面分别定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (time, previous_cell_output, previous_cell_state, previous_loop_state) -> \n",
    "#     (elements_finished, input, cell_state, output, loop_state).\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b) # projection layer\n",
    "        # [batch_size, vocab_size]\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        # [batch_size, input_embedding_size]\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    # input shape [batch_size,input_embedding_size]\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们分别定义了两个loop_fn，下面我们会将其合并为一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs # hidden_size = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了对输出做最后的projection操作，我们需要reshape操作\n",
    "\n",
    "[max_steps, batch_size, hidden_dim] to [max_steps*batch_size, hidden_dim]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "rnn的输出shape是：[max_time, batch_size, hidden_units]， 通过一个FC（projection layer）变换为[max_time, batch_size, vocab_size]，vocab_size是固定的，max_time and batch_size 是动态的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_targets:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(?, ?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "跟第一篇中的训练一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[3, 8, 9, 4, 9, 4]\n",
      "[2, 3, 7, 2, 5, 2]\n",
      "[4, 8, 8, 2]\n",
      "[9, 6, 2, 5, 8, 3, 5, 3]\n",
      "[9, 7, 2, 6]\n",
      "[7, 6, 2]\n",
      "[4, 9, 6, 6]\n",
      "[8, 4, 7, 8, 9, 8, 7]\n",
      "[4, 3, 6, 7, 4, 2]\n",
      "[6, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helper.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helper.batch(batch)\n",
    "    decoder_targets_, _ = helper.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.3008058071136475\n",
      "  sample 1:\n",
      "    input     > [2 7 7 4 0 0 0 0]\n",
      "    predicted > [8 8 0 0 8 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 2 6 3 2 4 4 0]\n",
      "    predicted > [8 9 9 9 9 9 9 9 9 9 0]\n",
      "  sample 3:\n",
      "    input     > [7 8 3 6 0 0 0 0]\n",
      "    predicted > [5 1 8 1 4 0 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.7816314697265625\n",
      "  sample 1:\n",
      "    input     > [4 7 4 4 0 0 0 0]\n",
      "    predicted > [4 4 4 4 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 5 4 5 5 3 7 0]\n",
      "    predicted > [4 5 5 5 5 5 5 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 6 7 7 0 0 0 0]\n",
      "    predicted > [6 7 7 7 1 0 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.37288352847099304\n",
      "  sample 1:\n",
      "    input     > [7 3 2 3 0 0 0 0]\n",
      "    predicted > [7 3 2 3 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 6 3 9 6 0 0 0]\n",
      "    predicted > [9 9 6 6 6 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 4 6 6 2 0 0 0]\n",
      "    predicted > [8 4 6 6 2 1 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.22249329090118408\n",
      "  sample 1:\n",
      "    input     > [6 3 5 0 0 0 0 0]\n",
      "    predicted > [6 3 5 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 2 5 5 2 4 0 0]\n",
      "    predicted > [3 2 5 5 2 4 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 7 3 0 0 0 0 0]\n",
      "    predicted > [2 7 3 1 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2228 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wdcSkFGUzfaOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121de2710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
