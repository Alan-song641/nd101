{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据论文[Sutskever, Vinyals and Le (2014)](https://arxiv.org/abs/1409.3215)而来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看图：\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2017/April/58e5893b_sequence-to-sequence-unrolled-encoder-decoder/sequence-to-sequence-unrolled-encoder-decoder.png)\n",
    "在实际的word2word中，我们会对单词进行embedding操作，此处为了简单起见，我们直接就以数字代表输入了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [[5, 7, 8], [6, 3], [3], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt, xlen = helper.batch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6, 3, 1],\n",
       "       [7, 3, 0, 0],\n",
       "       [8, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt # [max_time_len, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在处理中，我们会做一些特殊的处理\n",
    "\n",
    "1. < PAD>: 在训练过程中，batch中每个句子长度会不同了，此时我们对于短的就直接用 < PAD> 来填充的\n",
    "2. < EOS>: EOS代表的句子的结尾\n",
    "3. < UNK>: 对于一些不常见的词汇，直接用UNK替换掉（例如人名）\n",
    "4. < GO>: decode的第一个输入，告诉decode预测开始\n",
    "\n",
    "## 定义模型\n",
    "在定义模型的时候，我们需要确定的是 vocab_size ， input_embedding_size 和 encoder_hidden_units 和 decoder_hidden_units ，一旦修改得重新定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "# UNK = 2\n",
    "# GO  = 3\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个复杂的模型，我们想要去了解他，最好的方式就是看输入和输出，seq2seq的模型其输入和输出是：\n",
    "\n",
    "- encoder_inputs int32 tensor is shaped [encoder_max_time, batch_size]\n",
    "- decoder_targets int32 tensor is shaped [decoder_max_time, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还需要定义的一个输入是decoder的输入\n",
    "- decoder_inputs int32 tensor is shaped [decoder_max_time, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型训练的时候，对于decoder的ouputs我们不会使用，而是直接使用decoder_targets作为decoder的输入，但是在做predictions的时候，我们却会使用decoder的输出作为下一个lstm的输入，这可能会引入 distribution shift from training to prediction.\n",
    "\n",
    "## Embeddings\n",
    "我们系统的输入encoder_inputs和decoder_inputs都是 [decoder_max_time, batch_size]的形状，但是我们 encoder 和 decoder 的输入形状都是要 [max_time, batch_size, input_embedding_size]， 因此我们需要对我们的是输入做一个word embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.truncated_normal([vocab_size, input_embedding_size], mean=0.0, stddev=0.1), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(?, ?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_inputs_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.BasicLSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layers = 4\n",
    "cell = tf.contrib.rnn.MultiRNNCell([encoder_cell] * lstm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If `time_major == True`, this must be a `Tensor` of shape:\n",
    "#       `[max_time, batch_size, ...]`, or a nested tuple of such\n",
    "#       elements.\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell,encoder_inputs_embedded,dtype=tf.float32,time_major=True)\n",
    "del encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们直接删除了 encoder_outputs， 因为在这个场景中我们是不关注的，我们需要的是最后的 encoder_final_state，这又被称为 \"thought vector\"，如果没有引入attention机制，encoder_final_state 就是decoder的唯一输入，用他来作为decoder的init_state来解出decoder_targets。\n",
    "\n",
    ">We hope that backpropagation through time (BPTT) algorithm will tune the model to pass enough information throught the thought vector for correct sequence output decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 20) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 20) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_6:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_7:0' shape=(?, 20) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_8:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_9:0' shape=(?, 20) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "print(encoder_final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = tf.contrib.rnn.BasicLSTMCell(decoder_hidden_units)\n",
    "decoder = tf.contrib.rnn.MultiRNNCell([decoder_cell] * lstm_layers)\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "    decoder, decoder_inputs_embedded,\n",
    "    initial_state=encoder_final_state,\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们不关心decoder_inputs，而是关心decoder_outputs，对于decoder_outputs我们加一个fc，active_function=softmax，得到预测的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.fully_connected(decoder_outputs,vocab_size,activation_fn=None,\n",
    "                                              weights_initializer = tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                              biases_initializer=tf.zeros_initializer())\n",
    "# decoder_prediction = tf.argmax(decoder_logits,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fully_connected/Reshape_1:0\", shape=(?, ?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits,2) # 在这一步我突然意识到了axis的含义。。。表明的竟然是在哪个维度上求 argmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(?, ?), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于RNN的输出，其shape是：[max_time, batch_size, hidden_units]，通过一个FC，将其映射为：[max_time, batch_size, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn_rate = tf.placeholder(tf.float32)\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 试运行\n",
    ">deep learning is a game of shapes\n",
    "\n",
    "当我们build graph的时候，如果shape错误就马上会提示，但是一些其他的shape检查，只有我们运行的时候才会发现错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoded:\n",
      "[[6 3 9]\n",
      " [0 4 8]\n",
      " [0 0 7]]\n",
      "decoder inputs:\n",
      "[[1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "decoder predictions:\n",
      "[[9 6 6]\n",
      " [9 6 2]\n",
      " [9 6 2]\n",
      " [9 9 4]]\n",
      "build graph ok!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_ = [[6], [3, 4], [9, 8, 7]]\n",
    "\n",
    "    batch_, batch_length_ = helper.batch(batch_)\n",
    "    print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "    din_, dlen_ = helper.batch(np.ones(shape=(3, 1), dtype=np.int32),\n",
    "                                max_sequence_length=4)\n",
    "    print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "    pred_ = sess.run(decoder_prediction,\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_,\n",
    "            decoder_inputs: din_,\n",
    "#             learn_rate:0.1,\n",
    "        })\n",
    "    print('decoder predictions:\\n' + str(pred_))\n",
    "    \n",
    "print(\"build graph ok!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模拟训练\n",
    "我们为了简单起见，产生了随机的输入序列，然后decoder原模原样的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[7, 2, 9, 2, 2, 4, 4]\n",
      "[6, 9, 8, 5, 2, 3]\n",
      "[9, 3, 2, 4, 7]\n",
      "[2, 5, 3, 3, 6, 8, 9]\n",
      "[2, 4, 8, 5, 5, 3]\n",
      "[2, 6, 3]\n",
      "[3, 5, 2, 2]\n",
      "[9, 5, 3]\n",
      "[8, 5, 4, 2]\n",
      "[4, 9, 5, 2, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helper.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, _ = helper.batch(batch)\n",
    "    decoder_targets_, _ = helper.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, _ = helper.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        decoder_inputs: decoder_inputs_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当encoder_inputs 是[5, 6, 7]是decoder_targets是 [5, 6, 7, 1],1代表的是EOF，decoder_inputs则是 [1, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.301229476928711\n",
      "  sample 1:\n",
      "    input     > [4 8 3 3 4 8 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 8 7 8 4 3 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 4 3 0 0 0 0 0]\n",
      "    predicted > [6 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.958212673664093\n",
      "  sample 1:\n",
      "    input     > [7 2 6 8 0 0 0 0]\n",
      "    predicted > [7 7 3 3 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 6 8 6 3 8 0 0]\n",
      "    predicted > [3 3 6 6 6 6 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 2 4 4 0 0 0 0]\n",
      "    predicted > [5 4 4 4 1 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.3982703983783722\n",
      "  sample 1:\n",
      "    input     > [8 7 8 0 0 0 0 0]\n",
      "    predicted > [8 7 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 7 9 5 3 0 0 0]\n",
      "    predicted > [3 7 8 5 9 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 8 9 2 0 0 0 0]\n",
      "    predicted > [2 3 9 2 1 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        for batch in range(max_batches):\n",
    "            fd = next_feed()\n",
    "            _, l = sess.run([train_op, loss], fd)\n",
    "            loss_track.append(l)\n",
    "\n",
    "            if batch == 0 or batch % batches_in_epoch == 0:\n",
    "                print('batch {}'.format(batch))\n",
    "                print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "                predict_ = sess.run(decoder_prediction, fd)\n",
    "                for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    print('    input     > {}'.format(inp))\n",
    "                    print('    predicted > {}'.format(pred))\n",
    "                    if i >= 2:\n",
    "                        break\n",
    "                print()\n",
    "    except KeyboardInterrupt:\n",
    "        print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
