{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原地址：https://github.com/llSourcell/How-to-Use-Tensorflow-for-Time-Series-Live-/blob/master/demo_full_notes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*UkI9za9zTR-HL8uM15Wmzw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*UkI9za9zTR-HL8uM15Wmzw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\n",
    "num_epochs = 5\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 2, 2, 3, 1, 3, 3, 2, 1, 1, 1, 1, 3, 2, 1, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choice(a, size=None, replace=True, p=None)\n",
    "\n",
    "# Generates a random sample from a given 1-D array\n",
    "np.random.choice(4,20,p=[0.1,0.25,0.25,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 第一步，collect data\n",
    "# 输入是一个二元的vector data，而输出则是一个 echo_state，将数据左移echo_step产生\n",
    "\n",
    "def generateData():\n",
    "    #0,1, 50K samples, 50% chance each chosen\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    #shift 3 steps to the left\n",
    "    y = np.roll(x, echo_step)\n",
    "    #padd beginning 3 values with 0\n",
    "    y[0:echo_step] = 0\n",
    "    #Gives a new shape to an array without changing its data.\n",
    "    #The reshaping takes the whole dataset and puts it into a matrix, \n",
    "    #that later will be sliced up into these mini-batches.\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "data = generateData()\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data)[0].shape # x （5，10000）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 第二步构建模型\n",
    "#datatype, shape (5, 15) 2D array or matrix, batch size shape for later\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "#and one for the RNN state, 5,4 \n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00563077,  0.0242895 ,  0.10234547,  0.53831769],\n",
       "       [ 0.54123669,  0.19812725,  0.72408549,  0.47644339],\n",
       "       [ 0.11091712,  0.51419713,  0.12666874,  0.83239606],\n",
       "       [ 0.09600556,  0.20569164,  0.34628382,  0.69350262],\n",
       "       [ 0.1218765 ,  0.34860686,  0.66102863,  0.88259843]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(state_size+1, state_size) # (5, 4)\n",
    "# np.shape(np.random.rand(state_size+1, state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 layer recurrent net, one hidden state\n",
    "\n",
    "#randomly initialize weights\n",
    "# 此处为什么W是 state_size+1 呢？因为输入是 5*1 加上状态是 5 * 4，组合为输入就是 [5, 1 + 4]，因此 W 是 1 + state_size\n",
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32) \n",
    "#anchor, improves convergance, matrix of 0s \n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看一个例子：总共36个数，batch_size=3，然后truncated_backprop_length=3，每次处理完 truncated_backprop_length，向后移动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*n45uYnAfTDrBvG87J-poCA.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*n45uYnAfTDrBvG87J-poCA.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 我们将batch data转换到相邻的 time-steps\n",
    "# batchX_placeholder (5, 15) => inputs_series，将其组装为一列一列输出\n",
    "inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.,  2.,  3.,  4.], dtype=float32), array([ 2.,  4.,  6.,  8.], dtype=float32), array([  3.,   6.,   9.,  12.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# tf.unstack() 我们可以方便的进行将数据转换为列后输出\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = tf.placeholder(tf.float32, [4, 3])\n",
    "    data = tf.unstack(x,axis=1)\n",
    "    data_list = sess.run(data,feed_dict={\n",
    "        x:[[1,2,3],[2,4,6],[3,6,9],[4,8,12]]\n",
    "    })\n",
    "    print(data_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*f2iL4zOkBUBGOpVE7kyajg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*f2iL4zOkBUBGOpVE7kyajg.png\")\n",
    "#Schematic of the current batch split into columns, the order index is shown on each data-point \n",
    "#and arrows show adjacent time-steps.\n",
    "# 上面 unstack 作用就如下图所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 3 4]\n",
      " [2 2 4 6 8]\n",
      " [3 3 2 3 4]\n",
      " [4 4 2 3 4]\n",
      " [5 5 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "# 我们可以看到 axis=1 的时候就是行数不变，将后面一个并入\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    t1 = [[1],\n",
    "       [2],\n",
    "       [3],\n",
    "       [4],\n",
    "       [5]]\n",
    "    t2 = [[1,2,3,4],[2,4,6,8],[3,2,3,4],[4,2,3,4],[5,2,3,4]]\n",
    "    print(sess.run(tf.concat([t1,t2],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Forward pass\n",
    "#state placeholder\n",
    "current_state = init_state\n",
    "#series of states through time\n",
    "states_series = []\n",
    "\n",
    "\n",
    "#for each set of inputs\n",
    "#forward pass through the network to get new state value\n",
    "#store all states in memory\n",
    "for current_input in inputs_series:\n",
    "    #format input\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "    #mix both state and input data \n",
    "    # 变为了 [batch_size,1+state_size] [5,5]\n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state],1)  # Increasing number of columns\n",
    "    #perform matrix multiplication between weights and input, add bias\n",
    "    #squash with a nonlinearity, for probabiolity value\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "    #store the state in memory\n",
    "    states_series.append(next_state)\n",
    "    #set current state to next one\n",
    "    current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*fdwNNJ5UOE3Sx0R_Cyfmyg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*fdwNNJ5UOE3Sx0R_Cyfmyg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "此处 truncated_backprop_length 的作用是：RNN中不断将网络重复，但是如果我们一直将其保留在内存中，成本会非常大，因此我们需要适时的进行截断， truncated_backprop_length 就是保留在内存中的最大深度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate loss\n",
    "#second part of forward pass\n",
    "#logits short for logistic transform\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "#apply softmax nonlinearity for output probability\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.nn.sparse_softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算损失函数\n",
    "#measure loss, calculate softmax again on logits, then compute cross entropy\n",
    "#measures the difference between two probability distributions\n",
    "#this will return A Tensor of the same shape as labels and of the same type as logits \n",
    "#with the softmax cross entropy loss.\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "#computes average, one value\n",
    "total_loss = tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用的优化方法不是 SGD 而是 adagrad \n",
    "# SGD 的缺点是：对于学习rate比较敏感 \n",
    "# 每个 feature 都应该有一个不同的 learning rate\n",
    "# 有大梯度的应该有小的learning rate\n",
    "# 小梯度的则是大的learning rate\n",
    "# http://seed.ucsd.edu/mediawiki/images/6/6a/Adagrad.pdf\n",
    "\n",
    "#use adagrad to minimize with .3 learning rate\n",
    "#minimize it with adagrad, not SGD\n",
    "#One downside of SGD is that it is sensitive to\n",
    "#the learning rate hyper-parameter. When the data are sparse and features have\n",
    "#different frequencies, a single learning rate for every weight update can have\n",
    "#exponential regret.\n",
    "#Some features can be extremely useful and informative to an optimization problem but \n",
    "#they may not show up in most of the training instances or data. If, when they do show up, \n",
    "#they are weighted equally in terms of learning rate as a feature that has shown up hundreds \n",
    "#of times we are practically saying that the influence of such features means nothing in the \n",
    "#overall optimization. it's impact per step in the stochastic gradient descent will be so small \n",
    "#that it can practically be discounted). To counter this, AdaGrad makes it such that features \n",
    "#that are more sparse in the data have a higher learning rate which translates into a larger \n",
    "#update for that feature\n",
    "#sparse features can be very useful.\n",
    "#Each feature has a different learning rate which is adaptable. \n",
    "#gives voice to the little guy who matters a lot\n",
    "#weights that receive high gradients will have their effective learning rate reduced, \n",
    "#while weights that receive small or infrequent updates will have their effective learning rate increased. \n",
    "#great paper http://seed.ucsd.edu/mediawiki/images/6/6a/Adagrad.pdf\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [[1,1,0,1,0],[1,1,0,1,0],[1,1,0,1,0]]\n",
    "np.array(pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可视化函数\n",
    "# visualizer\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    # 2 行 3 列\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2]) # [xmin, xmax, ymin, ymax]\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\") # plt.bar(left, height, width=0.8, bottom=None\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蓝色代表输入\n",
    "Blue bars denote a training input signal (binary one)\n",
    "红色代表真实输出\n",
    "red bars show echos in the training output \n",
    "绿色是预测的值\n",
    "green bars are the echos the net is generating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-51-dce583b17198>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11066a3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data, epoch 0\n",
      "Step 0 Loss 0.688147\n",
      "Step 100 Loss 0.429072\n",
      "Step 200 Loss 0.0273838\n",
      "Step 300 Loss 0.0074923\n",
      "Step 400 Loss 0.00684299\n",
      "Step 500 Loss 0.00336111\n",
      "Step 600 Loss 0.00264705\n",
      "New data, epoch 1\n",
      "Step 0 Loss 0.200985\n",
      "Step 100 Loss 0.00177809\n",
      "Step 200 Loss 0.00198204\n",
      "Step 300 Loss 0.00151109\n",
      "Step 400 Loss 0.00161748\n",
      "Step 500 Loss 0.00124855\n",
      "Step 600 Loss 0.00113709\n",
      "New data, epoch 2\n",
      "Step 0 Loss 0.203546\n",
      "Step 100 Loss 0.00097241\n",
      "Step 200 Loss 0.00107177\n",
      "Step 300 Loss 0.000753635\n",
      "Step 400 Loss 0.000810073\n",
      "Step 500 Loss 0.000749434\n",
      "Step 600 Loss 0.000740377\n",
      "New data, epoch 3\n",
      "Step 0 Loss 0.220854\n",
      "Step 100 Loss 0.000735126\n",
      "Step 200 Loss 0.00060384\n",
      "Step 300 Loss 0.000532169\n",
      "Step 400 Loss 0.000571297\n",
      "Step 500 Loss 0.000684456\n",
      "Step 600 Loss 0.00045192\n",
      "New data, epoch 4\n",
      "Step 0 Loss 0.254147\n",
      "Step 100 Loss 0.000534385\n",
      "Step 200 Loss 0.000542578\n",
      "Step 300 Loss 0.000474023\n",
      "Step 400 Loss 0.000409939\n",
      "Step 500 Loss 0.000416673\n",
      "Step 600 Loss 0.000403649\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAVjyPvoGx4uXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11031c5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step 3 Training the network\n",
    "with tf.Session() as sess:\n",
    "    #we stupidly have to do this everytime, it should just know\n",
    "    #that we initialized these vars. v2 guys, v2..\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    #interactive mode\n",
    "    plt.ion()\n",
    "    #initialize the figure\n",
    "    plt.figure()\n",
    "    #show the graph\n",
    "    plt.show()\n",
    "    #to show the loss decrease\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        #generate data at eveery epoch, batches run in epochs\n",
    "        x,y = generateData()\n",
    "        #initialize an empty hidden state\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "        #each batch\n",
    "        for batch_idx in range(num_batches):\n",
    "            #starting and ending point per batch\n",
    "            #since weights reoccuer at every layer through time\n",
    "            #These layers will not be unrolled to the beginning of time, \n",
    "            #that would be too computationally expensive, and are therefore truncated \n",
    "            #at a limited number of time-steps\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "            \n",
    "            #run the computation graph, give it the values\n",
    "            #we calculated earlier\n",
    "            # 误差，梯度，当前输出状态，预测\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    init_state:_current_state\n",
    "                })\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "#                 (15, 5, 2) (5, 15) (5, 15)\n",
    "#                 print(loss_list,np.shape(_predictions_series),np.shape(batchX),np.shape(batchY))\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
