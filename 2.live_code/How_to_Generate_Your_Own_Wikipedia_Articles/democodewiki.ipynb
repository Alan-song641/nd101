{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "看了前面几个视频，给出的notebook都不是很好，都不能运行，终于这次视频给出的代码是可运行的了，本文主要是介绍怎么使用tf来对文章做summary，很cool的功能，也很实用，想想我自己读完一篇文章后都不能做总结，现在机器可以做了，真是令人兴奋\n",
    "\n",
    "本文使用的数据集是：https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "\n",
    "这些数据都是从Wikipedia的文章上解析出来的100million的tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import numpy as np #vectorization\n",
    "import random #generate probability distribution \n",
    "import tensorflow as tf #ml\n",
    "import datetime #clock training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "下一步我们将读取文章内容,数据的下载地址是：https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of characters: 1288556\n",
      "head of text:\n",
      " \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an English film , television and theatre actor . He had \n"
     ]
    }
   ],
   "source": [
    "with open('wiki.test.raw', 'rt',encoding='utf-8') as f:\n",
    "#     f.encoding = 'utf-8'\n",
    "    text = f.read()\n",
    "\n",
    "print('text length in number of characters:', len(text))\n",
    "\n",
    "print('head of text:')\n",
    "print(text[:100]) #all tokenized words, stored in a list called text\n",
    "text = text[:120000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "下一步我们要将数据进行排序，看有多少的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters: 127\n",
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'á', 'æ', 'è', 'é', 'í', 'ñ', 'ě', 'ī', 'Ō', 'ō', 'ū', 'ǐ', 'ǜ', '–', '—', '’', '−', '♯', '伊', '傳', '八', '勢', '史', '型', '士', '大', '律', '成', '戦', '春', '望', '杜', '甫', '聖', '艦', '處', '衛', '解', '詩', '贈', '邵', '集']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_size = len(chars)\n",
    "print('number of characters:', char_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "下一步我们要建立两个字典，一个是index => character，另一个是character => index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(prediction):\n",
    "    #Samples are uniformly distributed over the half-open interval \n",
    "    r = random.uniform(0,1)\n",
    "    #store prediction char\n",
    "    s = 0\n",
    "    #since length > indices starting at 0\n",
    "    char_id = len(prediction) - 1\n",
    "    #for each char prediction probabilty\n",
    "    for i in range(len(prediction)):\n",
    "        #assign it to S\n",
    "        s += prediction[i]\n",
    "        #check if probability greater than our randomly generated one\n",
    "        if s >= r:\n",
    "            #if it is, thats the likely next char\n",
    "            char_id = i\n",
    "            break\n",
    "    #dont try to rank, just differentiate\n",
    "    #initialize the vector\n",
    "    char_one_hot = np.zeros(shape=[char_size])\n",
    "    #that characters ID encoded\n",
    "    #https://image.slidesharecdn.com/latin-150313140222-conversion-gate01/95/representation-learning-of-vectors-of-words-and-phrases-5-638.jpg?cb=1426255492\n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "接着我们要做的是怎么将输入转换为向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "在lstm中都会有个连续输入的值，然后导出一个输出，形象点就是：\n",
    "![](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "在[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)文章中有介绍具体的模型，我们有多个输入来预测一个输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len_per_section = 50\n",
    "skip = 2\n",
    "sections = []\n",
    "next_chars = []\n",
    "#fill sections list with chunks of text, every 2 characters create a new 50 \n",
    "#character long section\n",
    "#because we are generating it at a character level\n",
    "for i in range(0, len(text) - len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an Eng \n",
      "Predictions: l\n"
     ]
    }
   ],
   "source": [
    "print(sections[0],\"\\nPredictions:\",next_chars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an Engli \n",
      "Predictions: s\n"
     ]
    }
   ],
   "source": [
    "print(sections[1],\"\\nPredictions:\",next_chars[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "我们可以看到sections[1]比sections[0]就是多移动了skip=2位，然后每len_per_section=50预测1个character\n",
    "\n",
    "下一步我们就是将输入和输出转换为one-hot encoded的形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(sections), len_per_section, char_size))\n",
    "#label column for all the character id's, still zero\n",
    "y = np.zeros((len(sections), char_size))\n",
    "#for each char in each section, convert each char to an ID\n",
    "#for each section convert the labels to ids \n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1\n",
    "    \n",
    "## 如果too large 就会出现问题     MemoryError: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "batch size的意义：理论上我们应该将所有的数据都放入模型中计算后，然后通过梯度方法进行更新，但是这样带来的问题是：内存消耗太大了，于是我们就有了部分数据后就进行梯度更新的方法，这个batch size就是一次更新中用到的数据\n",
    "\n",
    "> batch size = the number of training examples in one forward/backward pass\n",
    "\n",
    "该值越大，消耗的内存也就越多\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "#total iterations\n",
    "max_steps = 72001\n",
    "#how often to log?\n",
    "log_every = 100\n",
    "#how often to save?\n",
    "save_every = 6000\n",
    "\n",
    "#too few and underfitting\n",
    "#Underfitting occurs when there are too few neurons \n",
    "#in the hidden layers to adequately detect the signals in a complicated data set.\n",
    "#too many and overfitting\n",
    "hidden_nodes = 1024\n",
    "#starting text\n",
    "test_start = 'I am thinking that'\n",
    "#to save our model\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 59975\n",
      "approximate steps per epoch: 117\n"
     ]
    }
   ],
   "source": [
    "#Create a checkpoint directory\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per epoch:', int(len(X)/batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "下面我们会用tf的基本函数来构建lstm模型，关于lstm模型的理解我之前写过一篇文章：[lstm的理解](http://www.jianshu.com/p/2f33b3040b50)\n",
    "\n",
    "最重要的概念是我们要实现3个门：\n",
    "1. 遗忘门（forget gate）\n",
    "2. 输入门（input gate）\n",
    "3. 输出门（output gate）\n",
    "![](http://upload-images.jianshu.io/upload_images/2256672-bff9353b92b9c488.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 遗忘门（forget gate）\n",
    "遗忘门控制着以前的记忆有多少进行保存\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*9ZgZxRSVAvJEsaoWtz69yg.png)\n",
    "\n",
    "针对这个图我们就有定义：\n",
    "\n",
    "```\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "  \n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 输入门\n",
    "输入门控制着当前输入有多少进行记忆\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*lNDzNHVxLSKJEpCsP4KD4w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 输出门\n",
    "输出门控制着当前状态有多少输出到输出\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*mOhp-z4KM0Qm6Y0RY7YgMQ.png)\n",
    "\n",
    "\n",
    "下面我们来看完整的代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "!note：关于初始化参数的选择问题可以看：[weight_initialization](https://github.com/zhuanxuhit/deep-learning/blob/4427bf70640baa16b9d0f4ffa2acdcff8dc7b50a/weight-initialization/weight_initialization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # 记录着当前步数\n",
    "    global_step = tf.Variable(0)\n",
    "    #data tensor shape feeding in sections\n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "    #labels\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "    # 记录各个参数，用 truncated_normal 来记录\n",
    "    # input gate\n",
    "    w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Forget gate: weights for input, weights for previous output, and bias\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Output gate: weights for input, weights for previous output, and bias\n",
    "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Memory cell: weights for input, weights for previous output, and bias\n",
    "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    def lstm(i, o, state):\n",
    "        \n",
    "        #these are all calculated seperately, no overlap until....\n",
    "        #(input * input weights) + (output * weights for previous output) + bias\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        #(input * forget weights) + (output * weights for previous output) + bias\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        #(input * output weights) + (output * weights for previous output) + bias\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        #(input * internal state weights) + (output * weights for previous output) + bias\n",
    "        memory_cell = tf.tanh(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        \n",
    "        #...now! multiply forget gate * given state    +  input gate * hidden state\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        #squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
    "        #multiply by output\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        #return \n",
    "        return output, state\n",
    "    \n",
    "    # 具体的lstm的执行\n",
    "    #both start off as empty, LSTM will calculate this\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "    \n",
    "    #for each input set\n",
    "    for i in range(len_per_section):\n",
    "        #calculate state and output from LSTM\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        #to start, \n",
    "        if i == 0:\n",
    "            #store initial output and labels\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        #for each new set, concat outputs and labels\n",
    "        elif i != len_per_section - 1:\n",
    "            #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, data[:, i+1, :]], 0)\n",
    "        else:\n",
    "            #final store\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, labels],0)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "当我们计算完lstm后，我们要对输出在做个FC层，将其转换为我们需要的数据，即one-hot encoded的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_all_i))\n",
    "    # sparse_softmax_cross_entropy_with_logits 是当labels是排外的，只有一个是有值的时候使用\n",
    "    # loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_all_i))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ###########\n",
    "    #Test\n",
    "    ###########\n",
    "    test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #Reset at the beginning of each test\n",
    "    reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #LSTM\n",
    "    test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 训练\n",
    "模型都定义好了，下面就是进行训练了，此处对于最后的一些训练数据，我们循环的用开头的数据进行补全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 5.21 (2017-03-28 08:29:48.254496)\n",
      "training loss at step 100: 3.58 (2017-03-28 08:31:23.580246)\n",
      "training loss at step 200: 3.26 (2017-03-28 08:32:57.851588)\n",
      "training loss at step 300: 3.17 (2017-03-28 08:34:32.126792)\n",
      "training loss at step 400: 3.41 (2017-03-28 08:36:06.404104)\n",
      "training loss at step 500: 3.13 (2017-03-28 08:37:40.683548)\n",
      "training loss at step 600: 2.94 (2017-03-28 08:39:14.970380)\n",
      "training loss at step 700: 2.97 (2017-03-28 08:40:49.246994)\n",
      "training loss at step 800: 3.01 (2017-03-28 08:42:23.517853)\n",
      "training loss at step 900: 3.04 (2017-03-28 08:43:57.792229)\n",
      "training loss at step 1000: 2.88 (2017-03-28 08:45:32.057134)\n",
      "training loss at step 1100: 2.95 (2017-03-28 08:47:06.328897)\n",
      "training loss at step 1200: 2.93 (2017-03-28 08:48:40.599889)\n",
      "training loss at step 1300: 2.82 (2017-03-28 08:50:14.861270)\n",
      "training loss at step 1400: 2.91 (2017-03-28 08:51:49.130109)\n",
      "training loss at step 1500: 2.86 (2017-03-28 08:53:23.395290)\n",
      "training loss at step 1600: 2.99 (2017-03-28 08:54:57.663434)\n",
      "training loss at step 1700: 2.86 (2017-03-28 08:56:31.943277)\n",
      "training loss at step 1800: 2.89 (2017-03-28 08:58:06.225545)\n",
      "training loss at step 1900: 3.00 (2017-03-28 08:59:40.498702)\n",
      "training loss at step 2000: 2.79 (2017-03-28 09:01:14.777135)\n",
      "training loss at step 2100: 2.80 (2017-03-28 09:02:49.047066)\n",
      "training loss at step 2200: 2.93 (2017-03-28 09:04:23.326559)\n",
      "training loss at step 2300: 2.84 (2017-03-28 09:05:57.597121)\n",
      "training loss at step 2400: 2.80 (2017-03-28 09:07:31.875869)\n",
      "training loss at step 2500: 2.97 (2017-03-28 09:09:06.137279)\n",
      "training loss at step 2600: 2.78 (2017-03-28 09:10:40.402929)\n",
      "training loss at step 2700: 2.83 (2017-03-28 09:12:14.675483)\n",
      "training loss at step 2800: 2.83 (2017-03-28 09:13:48.945418)\n",
      "training loss at step 2900: 2.83 (2017-03-28 09:15:23.212972)\n",
      "training loss at step 3000: 2.77 (2017-03-28 09:16:57.493981)\n",
      "training loss at step 3100: 2.90 (2017-03-28 09:18:31.766771)\n",
      "training loss at step 3200: 2.95 (2017-03-28 09:20:06.029838)\n",
      "training loss at step 3300: 2.83 (2017-03-28 09:21:40.290033)\n",
      "training loss at step 3400: 2.92 (2017-03-28 09:23:14.560192)\n",
      "training loss at step 3500: 2.75 (2017-03-28 09:24:48.826512)\n",
      "training loss at step 3600: 2.91 (2017-03-28 09:26:23.089663)\n",
      "training loss at step 3700: 2.89 (2017-03-28 09:27:57.358097)\n",
      "training loss at step 3800: 3.07 (2017-03-28 09:29:31.625090)\n",
      "training loss at step 3900: 2.96 (2017-03-28 09:31:05.910301)\n",
      "training loss at step 4000: 2.85 (2017-03-28 09:32:40.227497)\n",
      "training loss at step 4100: 2.85 (2017-03-28 09:34:14.530017)\n",
      "training loss at step 4200: 2.81 (2017-03-28 09:35:48.824100)\n",
      "training loss at step 4300: 2.91 (2017-03-28 09:37:23.097314)\n",
      "training loss at step 4400: 2.91 (2017-03-28 09:38:57.375260)\n",
      "training loss at step 4500: 3.12 (2017-03-28 09:40:31.644089)\n",
      "training loss at step 4600: 2.85 (2017-03-28 09:42:05.912328)\n",
      "training loss at step 4700: 2.83 (2017-03-28 09:43:40.184867)\n",
      "training loss at step 4800: 2.81 (2017-03-28 09:45:14.431610)\n",
      "training loss at step 4900: 2.88 (2017-03-28 09:46:48.703220)\n",
      "training loss at step 5000: 2.90 (2017-03-28 09:48:22.980590)\n",
      "training loss at step 5100: 2.83 (2017-03-28 09:49:57.254094)\n",
      "training loss at step 5200: 2.79 (2017-03-28 09:51:31.530851)\n",
      "training loss at step 5300: 2.83 (2017-03-28 09:53:05.804013)\n",
      "training loss at step 5400: 2.75 (2017-03-28 09:54:40.083138)\n",
      "training loss at step 5500: 2.81 (2017-03-28 09:56:14.346421)\n",
      "training loss at step 5600: 2.77 (2017-03-28 09:57:48.619380)\n",
      "training loss at step 5700: 2.94 (2017-03-28 09:59:22.891043)\n",
      "training loss at step 5800: 2.79 (2017-03-28 10:00:57.162585)\n",
      "training loss at step 5900: 2.81 (2017-03-28 10:02:31.426376)\n",
      "training loss at step 6000: 2.97 (2017-03-28 10:04:05.695705)\n",
      "training loss at step 6100: 2.73 (2017-03-28 10:05:40.937724)\n",
      "training loss at step 6200: 2.71 (2017-03-28 10:07:15.194703)\n",
      "training loss at step 6300: 2.86 (2017-03-28 10:08:49.485094)\n",
      "training loss at step 6400: 2.75 (2017-03-28 10:10:23.755326)\n",
      "training loss at step 6500: 2.70 (2017-03-28 10:11:58.037508)\n",
      "training loss at step 6600: 2.92 (2017-03-28 10:13:32.307874)\n",
      "training loss at step 6700: 2.72 (2017-03-28 10:15:06.581715)\n",
      "training loss at step 6800: 2.75 (2017-03-28 10:16:40.850969)\n",
      "training loss at step 6900: 2.75 (2017-03-28 10:18:15.109236)\n",
      "training loss at step 7000: 2.76 (2017-03-28 10:19:49.371711)\n",
      "training loss at step 7100: 2.70 (2017-03-28 10:21:23.638501)\n",
      "training loss at step 7200: 2.81 (2017-03-28 10:22:57.899049)\n",
      "training loss at step 7300: 2.91 (2017-03-28 10:24:32.163239)\n",
      "training loss at step 7400: 2.71 (2017-03-28 10:26:06.444371)\n",
      "training loss at step 7500: 2.82 (2017-03-28 10:27:40.695465)\n",
      "training loss at step 7600: 2.63 (2017-03-28 10:29:14.938830)\n",
      "training loss at step 7700: 2.80 (2017-03-28 10:30:49.201444)\n",
      "training loss at step 7800: 2.78 (2017-03-28 10:32:23.459906)\n",
      "training loss at step 7900: 2.90 (2017-03-28 10:33:57.726020)\n",
      "training loss at step 8000: 2.81 (2017-03-28 10:35:32.058897)\n",
      "training loss at step 8100: 2.74 (2017-03-28 10:37:06.365626)\n",
      "training loss at step 8200: 2.80 (2017-03-28 10:38:40.693417)\n",
      "training loss at step 8300: 2.67 (2017-03-28 10:40:15.001199)\n",
      "training loss at step 8400: 2.78 (2017-03-28 10:41:49.309667)\n",
      "training loss at step 8500: 2.80 (2017-03-28 10:43:23.618866)\n",
      "training loss at step 8600: 2.99 (2017-03-28 10:44:57.933725)\n",
      "training loss at step 8700: 2.70 (2017-03-28 10:46:32.253583)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-2cef50f3f74c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#optimize!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#timew to train the model, initialize a session with a graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #standard init step\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #for each training step\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #starts off as 0\n",
    "        offset = offset % len(X)\n",
    "        \n",
    "        #calculate batch data and labels to feed model iteratively\n",
    "        if offset <= (len(X) - batch_size):\n",
    "            #first part\n",
    "            batch_data = X[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        #until when offset  = batch size, then we \n",
    "        else:\n",
    "            #last part\n",
    "            to_add = batch_size - (len(X) - offset)\n",
    "            batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        \n",
    "        #optimize!!\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "\n",
    "            if step % save_every == 0:\n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "！！啃爹的我花了这么长时间训练出来的是什么鬼\n",
    "training loss at step 0: 5.87 (2017-03-28 07:40:33.699669)\n",
    "training loss at step 3000: 3292.38 (2017-03-28 08:27:45.663173)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "！！啃爹的我又训练了这么长时间，但是没有什么好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 预测\n",
    "最后我们来给定一段文字，看输出结果的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I plan to make the world a better place ie @wis E -urmalonthed 7 Kn . nthevig Wonghilofonsed ig blanan icake th Uin , s larshompaughemshonsimisinth pane shelanf pa , n h teag Kon , A FHor patral Fannodenitas cive )at @ \" an asthonte vid hin thicinge s ing tin ie s W thisas th t wainga ag Fe Yo , reō thed anng s afan Abuanthent is Pell \n",
      " cj Sin Ut 8uy anon amasthadithe wasulis en wanditre oo tarmm gheg ctohophiberPeces ke tofingutle hme bos mpg \n",
      " lan . thin fath Fosin heebu asicogang fthed eskissun oEmicthe ounbed in Ban s9 Suntlliend \n"
     ]
    }
   ],
   "source": [
    "test_start = 'I plan to make the world a better place '\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #init graph, load model\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    #set input variable to generate chars from\n",
    "    reset_test_state.run() \n",
    "    test_generated = test_start\n",
    "\n",
    "    #for every char in the input sentennce\n",
    "    for i in range(len(test_start) - 1):\n",
    "        #initialize an empty char store\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        #store it in id from\n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        #feed it to model, test_prediction is the output value\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    \n",
    "    #where we store encoded char predictions\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    #lets generate 500 characters\n",
    "    for i in range(500):\n",
    "        #get each prediction probability\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        # one hot encode it\n",
    "        # 前一步，我们将我们的数据进行了导入，但是这种预测方法让我不可接受\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        # print(prediction.shape) # (127,)\n",
    "        #get the indices of the max values (highest probability)  and convert to char\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #add each char to the output text iteratively\n",
    "        test_generated += next_char\n",
    "        #update the \n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
